Lesson 8.3 ID3、C4.5决策树的建模流程
ID3和C4.5作为的经典决策树算法，尽管无法通过sklearn来进行建模，但其基本原理仍然值得讨论
与学习。接下来我们详细介绍关于ID3和C4.5这两种决策树模型的建模基本思路和原理。ID3和C4.5的基
本建模流程和CART树是类似的，也是根据纯度评估指标选取最佳的数据集划分方式，只是不过ID3和
C4.5是以信息熵为评估指标，而数据集的离散特征划分方式也是一次展开一列，而不是寻找切点进行切
分。我们先从ID3的基本原理开始介绍，随后讨论C4.5在ID3基础上的改善措施。
import numpy as np
from ML_basic_function import *
一、ID3决策树的基本建模流程
ID3是一个只能围绕离散型变量进行分类问题建模的决策树模型，即ID3无法处理连续型特征、也无
法处理回归问题，如果带入训练数据有连续型变量，则首先需要对其进行离散化处理，也就是连续变量
分箱。例如如下个人消费数据，各特征都是离散型变量，能够看出，其中age和income两列就是连续型
变量分箱之后的结果，例如age列就是以30、40为界进行连续变量分箱。当然，除了如下表示外，我们
还可以将分箱之后的结果直接赋予一个离散的值，如1、2、3等。
更多关于连续变量的离散化的方法将在特征工程部分进行介绍。
ID3的生长过程其实和CART树基本一致，其目标都是尽可能降低数据集的不纯度，其生长的过程也
就是数据集不断划分的过程。只不过ID3的数据集划分过程（规律提取过程）和CART树有所不同，CART
树是在所有特征中寻找切分点、然后再从中挑选出能够最大程度降低数据集不纯度的节分方式，换而言
之就是CART树是按照某切分点来展开，而ID3则是按照列来展开，即根据某列的不同取值来对数据集进
行划分。例如根据上述数据集中的age列的不同取值来对原始数据集进行划分，则划分结果如下：
同样，我们可以计算在以age的不同取值为划分规则、对数据集进行划分后数据集整体不纯度下降结果，
ID3中采用信息熵作为评估指标，具体计算过程如下：
首先计算父节点的信息熵
# 父节点A的信息熵
ent_A = -5/14 * np.log2(5/14) - 9/14 * np.log2(9/14)
ent_A
0.9402859586706311
然后计算每个子节点的信息熵
# 子节点B的信息熵
ent_B1 = entropy(2/5)
ent_B2 = entropy(2/5)
ent_B3 = 0
ent_B1, ent_B2, ent_B3

(0.9709505944546686, 0.9709505944546686, 0)
同样，子节点整体信息熵就是每个子节点的信息熵加权求和计算得出，其权重就是各子节点数据集数量
占父节点总数据量的比例：
ent_B = ent_B1 * 5/14 + ent_B2 * 5/14 + ent_B3 * 4/14
ent_B
0.6935361388961919
然后即可算出按照如此规则进行数据集划分，最终能够减少的不纯度数值：
# 不纯度下降结果
ent_A - ent_B
0.24674981977443922
而该结果也被称为根据age列进行数据集划分后的信息增益（information gain），上述结果可写成
Gain(age) = 0.247
当然，至此我们只计算了按照age列的不同取值来进行数据集划分后数据集不纯度下降结果，而按照
age列进行展开只能算是树的第一步生长中的一个备选划分规则，此外我们还需要测试按照income、
student或者credit_rating列展开后数据集不纯度下降情况，具体计算过程和age列展开后的计算过程类
似，此处直接给出结果，Gain(income)=0.026、Gain(student)=0.151、Gain(credit_rating)=0.048。很
明显，按照age列展开能够更有效的降低数据集的不纯度，因此树的第一层生长就是按照age列的不同取
值对数据集进行划分。
接下来需要继续进行迭代，通过观察我们不难发现，对于数据集B1来说来说，按照student这一列来
进行展开，能够让子节点的信息熵归零，而数据集B2按照如果按照credit_rating来展开，也同样可以将
子节点的标签纯度提高至100%。因此该模型最终树的生长形态如下：
至此，我们就完成了ID3决策树的建模全流程，具体模型结果解读和CART树完全一样，此处不做赘
述。接下来简单对比ID3和CART树之间的差异：首先，由于ID3是按照列来提取规则、每次展开一列，因
此每一步生长会有几个分支，其实完全由当前列有几个分类水平决定，而CART树只能进行二叉树的生
长；其次，由于ID3每次展开一列，因此建模过程中对“列的消耗”非常快，数据集中特征个数就决定了树
的最大深度，相比之下CART树的备选规则就要多的多，这也使得CART树能够进行更加精细的规则提
取；当然，尽管CART树和ID3存在着基本理论层面的差异，但有的时候也能通过CART树的方法来挖掘出
和ID3决策树相同的规律，例如ID3中按照age列一层展开所提取出的三个分类规则，也可以在CART树中
通过两层树来实现，例如第一层按照是否是<=30来进行划分、第二层围绕不满足第一层条件的数据集进
一步根据是否>40来进行划分。

此外，需要注意的是，正因为ID3是按照列来进行展开，因此只能处理特征都是离散变量的数据集。
另外，根据ID3的建模规则我们不难发现，ID3树在实际生长过程中会更倾向于挑选取值较多的分类变量
展开，但如此一来便更加容易造成模型过拟合，而遗憾的是ID3并没有任何防止过拟合的措施。而这些
ID3的缺陷，则正是C4.5算法的改进方向。接下来我们继续讨论关于C4.5决策树的建模规则。
当然，对于ID3来说，规则是和分类变量的取值一一绑定的，
二、C4.5决策树的基本建模流程
作为ID3的改进版算法，C4.5在ID3的基础上进行了三个方面的优化，首先在衡量不纯度降低的数值
计算过程中引入信息值（information value，也被称为划分信息度、分支度等）概念来修正信息熵的计
算结果，以抑制ID3更倾向于寻找分类水平较多的列来展开的情况，从而间接抑制模型过拟合倾向；其二
则是新增了连续变量的处理方法，也就是CART树中寻找相邻取值的中间值作为切分点的方法；其三是加
入了决策树的剪枝流程，使得模型泛化能力能够得到进一步提升。但需要注意的是，尽管有如此改进，
但C4.5仍然只能解决分类问题，其本质仍然还是一种分类树。接下来我们详细讨论C4.5的具体改进策
略。
信息值（information value）
C4.5中信息值（以下简称IV值）是一个用于衡量数据集在划分时分支个数的指标，如果划分时分支
越多，IV值就越高。具体IV值的计算公式如下：
IV值计算公式和信息熵的计算公式基本一致，只是具体计算的比例不再是各类样本所占比例，而是
各划分后子节点的数据所占比例，或者说信息熵是计算标签不同取值的混乱程度，而IV值就是计算
特征不同取值的混乱程度
其中K表示某次划分是总共分支个数，$v_i$表示划分后的某样本，$P(v_i)$表示该样本数量占父节点数据
量的比例。对于如下三种数据集划分情况，简单计算IV值：
# 父节点按照50%-50%进行划分
- (1/2 * np.log2(1/2) + 1/2 * np.log2(1/2))
1.0
# 父节点按照1/4-1/2-1/4进行划分
- (1/4 * np.log2(1/4) + 1/2 * np.log2(1/2)+ 1/4 * np.log2(1/4))
1.5

# 父节点按照1/4-1/4-1/4-1/4进行划分
- (1/4 * np.log2(1/4) + 1/4 * np.log2(1/4) + 1/4 * np.log2(1/4) + 1/4 *
np.log2(1/4))
2.0
而在实际建模过程中，ID3是通过信息增益的计算结果挑选划分规则，而C4.5采用IV值对信息增益计算结
果进行修正，构建新的数据集划分评估指标：增益比例（Gain Ratio，被称为获利比例或增益率），来指
导具体的划分规则的挑选。GR的计算公式如下：
也就是说，在C4.5的建模过程中，需要先计算GR，然后选择GR计算结果较大的列来执行这一次展
开。例如对于上述例子来看，以age列展开后Information Gain结果为：
IG = ent_A - ent_B
IG
0.24674981977443922
而IV值为：
IV = - (5/14 * np.log2(5/14) + 5/14 * np.log2(5/14)+ 4/14 * np.log2(4/14))
IV
1.5774062828523454
因此计算得到GR值为：
GR = IG / IV
GR
0.1564275624211752
然后据此进一步计算其他各列展开后的GR值，并选择GR较大者进行数据集划分。

C4.5的连续变量处理方法
C4.5允许带入连续变量进行建模，并且围绕连续变量的规则提取方式和此前介绍的CART树一致。即
在连续变量中寻找相邻的取值的中间点作为备选切分点，通过计算切分后的GR值来挑选最终数据集划分
方式。当然，基于连续变量的备选切分方式也要和离散变量的切分方式进行横向比较，到底是一次展开
一个离散列还是按照连续变量的某个切点展开，要根据最终GR的计算结果来决定。
例如，如果将上述数据集的age列换成连续变量，则我们需要计算的GR情况就变成了GR(income)、
GR(student)、GR(credit_rating)、GR(age<=26.5)、GR(age<=27.5)...
当然，由于C4.5的离散变量和连续变量提取规则方式不同，离散变量是一次消耗一列来进行展开
（有可能多分叉），而连续变量则一次消耗一个切分点，因此和CART树一样、同一个连续变量可
以多次指导数据集进行划分。
在sklearn的树模型介绍文档中，有一段关于sklearn的决策树不支持离散变量建模的说明，其意为
不支持按照类似ID3或C4.5的方式直接将离散变量按列来进行展开，而是根据sklearn中集成的
CART树自身的建模规则，使得sklearn中的决策树实际上在处理特征时都是按照C4.5中连续变量的
处理方式在进行处理，并非指的是带入离散变量就无法建模。

